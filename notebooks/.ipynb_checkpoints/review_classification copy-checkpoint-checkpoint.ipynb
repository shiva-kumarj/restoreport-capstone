{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\My-Projects\\stonecap\\data\\processed\\reviews\\chunk_0.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\n",
    "        \"customer_experience\",\n",
    "        \"food_quality\",\n",
    "        \"operations\",\n",
    "        \"staff_and_service\",\n",
    "        \"order_related\",\n",
    "    ]\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# classification_report = classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = df['statement'].tolist()\n",
    "classification_reports = classifier(statements[:100], candidate_labels)\n",
    "print(classification_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use tqdm to create a progress bar\n",
    "with tqdm(total=len(statements), desc=\"Classifying Statements\") as pbar:\n",
    "    # df['label'] = [report['labels'][0] for report in classification_reports]\n",
    "    pbar.update(len(statements))  # Update progress bar to indicate completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-11T20:23:13.220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blahb\\anaconda3\\envs\\capstone_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-11 16:23:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 2.85MB/s]                                                     \n",
      "2024-03-11 16:23:20 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-03-11 16:23:20 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2024-03-11 16:23:20 WARNING: GPU requested, but is not available!\n",
      "2024-03-11 16:23:20 INFO: Using device: cpu\n",
      "2024-03-11 16:23:20 INFO: Loading: tokenize\n",
      "2024-03-11 16:23:21 INFO: Loading: mwt\n",
      "2024-03-11 16:23:21 INFO: Loading: sentiment\n",
      "2024-03-11 16:23:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import stanza\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "log_file_path = os.path.abspath(\"D:\\My-Projects\\stonecap\\logs\\lemmatization.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "nlp = stanza.Pipeline(processors=\"tokenize,sentiment\", lang=\"en\", use_gpu=True)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(last_checkpoint):\n",
    "    with open(\"checkpoint.pkl\", \"wb\") as f:\n",
    "        pickle.dump(last_checkpoint, f)\n",
    "\n",
    "def load_checkpoint():\n",
    "    try:\n",
    "        with open(\"checkpoint.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return 0  # Start from the beginning if the checkpoint file doesn't exist\n",
    "\n",
    "def validate_datetime(chunk):\n",
    "    assert (\n",
    "        not chunk[\"date\"].isnull().any()\n",
    "    ), 'AssertionError: Null values found in the \"date\" column'\n",
    "    assert (\n",
    "        chunk[\"date\"].dtype == \"datetime64[ns]\"\n",
    "    ), \"AssertionError: dtype mismatch of date column\"\n",
    "    assert not (\n",
    "        (chunk[\"date\"].dt.month > 12) | (chunk[\"date\"].dt.month < 1)\n",
    "    ).any(), \"AssertionError: Month should be between 1 and 12 (inclusive)\"\n",
    "    assert not (\n",
    "        (chunk[\"date\"].dt.day > 31) | (chunk[\"date\"].dt.day < 1)\n",
    "    ).any(), \"AssertionError: Date should be between 1 and 31 (inclusive)\"\n",
    "\n",
    "\n",
    "def validate_numerical_col(chunk):\n",
    "    numerical_col = [\"stars\", \"useful\"]\n",
    "    for col in numerical_col:\n",
    "        assert (\n",
    "            chunk[col].dtype == \"int64\"\n",
    "        ), f\"AssertionError: {col} should have data type 'int64'\"\n",
    "        assert (\n",
    "            not chunk[col].isnull().any()\n",
    "        ), f\"AssertionError: {col} should not have missing values\"\n",
    "\n",
    "    assert (\n",
    "        1.0 <= chunk[\"stars\"].min() <= 5.0\n",
    "    ), \"AssertionError: 'stars' should be in the range of 1 to 5\"\n",
    "    assert (\n",
    "        1.0 <= chunk[\"stars\"].max() <= 5.0\n",
    "    ), \"AssertionError: 'stars' should be in the range of 1 to 5\"\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_data(chunk):\n",
    "    assert (\n",
    "        not chunk[\"business_id\"].isnull().any()\n",
    "    ), \"AssertionError: 'Business id' should not have missing values\"\n",
    "    validate_numerical_col(chunk)\n",
    "    validate_datetime(chunk)\n",
    "    assert (\n",
    "        not chunk.isnull().any().any()\n",
    "    ), \"AssertionError: Chunk must not contain any missing values\"\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_text(inp_text):\n",
    "    # lower case\n",
    "    inp_text = inp_text.lower()\n",
    "    # extract out all alphabet, numbers, and select special characters and join them back together with a 'space'.\n",
    "    regex_pattern = r'[a-zA-Z0-9s!?.\" \"]+'\n",
    "    matched_substrings = re.findall(regex_pattern, inp_text)\n",
    "    cleaned_text = \"\".join(matched_substrings)\n",
    "    # replace all non alphabetic, space and period characters with a period\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s.]\", \".\", cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_stars(stars_column):\n",
    "    # choice_of_ratings = [np.floor(stars_column.mean()), stars_column.mode(), stars_column.quantile(0.5)]\n",
    "    choice_of_ratings = [3, 4, 5]\n",
    "    stars_column = stars_column.fillna(np.random.choice(choice_of_ratings))\n",
    "    stars_column = stars_column.astype(\"int\")\n",
    "    return stars_column\n",
    "\n",
    "\n",
    "def clean_useful(useful_column):\n",
    "    useful_column = useful_column.fillna(useful_column.mode())\n",
    "    useful_column = useful_column.astype(\"int\")\n",
    "    return useful_column\n",
    "\n",
    "\n",
    "def clean_date(date_column):\n",
    "    date_column = pd.to_datetime(date_column)\n",
    "    return date_column\n",
    "\n",
    "\n",
    "def clean_data(chunk):\n",
    "    chunk.loc[:, \"business_id\"] = chunk[\"business_id\"].dropna()\n",
    "    chunk.loc[:, \"stars\"] = clean_stars(chunk[\"stars\"])\n",
    "    chunk.loc[:, \"useful\"] = clean_useful(chunk[\"useful\"])\n",
    "    chunk.loc[:, \"date\"] = clean_date(chunk[\"date\"])\n",
    "    chunk.loc[:, \"text\"] = chunk[\"text\"].apply(lambda x: clean_text(x))\n",
    "    return chunk\n",
    "\n",
    "\n",
    "# 1. ~~Spell correction (did not add any value, very time taking)~~\n",
    "# 2. ~~(Sentence, Sentiment) map using Stanza.~~\n",
    "# 3. ~~Flatten (sentence, sentiment) map into individual rows.~~\n",
    "# 4. annotate sentence into one word \"business area\" using OpenAI or other libraries available on github.\n",
    "#    1. Note: OpenAI is paid but gave the best results so far. better than manual labelling, semi-supervised labelling, and stanza.\n",
    "\n",
    "\n",
    "# # Retrieve the sentiment of each noun from a sentence\n",
    "# def get_line_sentiment(\n",
    "#     review,\n",
    "# ):\n",
    "#     sentiment_map = {}\n",
    "#     for sentence in nlp(review).sentences:\n",
    "#         sentiment = sentence.sentiment\n",
    "#         sentiment_map[sentence.text] = sentiment\n",
    "#     # record_count[0] += 1\n",
    "#     # logging.info(f'get_line_sentiment: {record_count[0]}/{record_count[1]}.')\n",
    "#     return sentiment_map\n",
    "\n",
    "def get_review_sentiment_stanza(review):\n",
    "    doc = nlp(review)\n",
    "    sentiment = doc.sentences[0].sentiment\n",
    "    return sentiment\n",
    "\n",
    "# def get_sentiment_noun(sentiment_dict):\n",
    "#     return sentiment_dict.keys()\n",
    "\n",
    "\n",
    "# def get_sentiment_value(sentiment_dict):\n",
    "#     return sentiment_dict.values()\n",
    "\n",
    "\n",
    "# def unpack_sentiment(df):\n",
    "#     df.loc[:, \"statement\"] = df[\"sentiment_dict\"].apply(get_sentiment_noun)\n",
    "#     df.loc[:, \"sentiment\"] = df[\"sentiment_dict\"].apply(get_sentiment_value)\n",
    "\n",
    "#     df = df.explode([\"statement\", \"sentiment\"])\n",
    "\n",
    "#     df.drop([\"sentiment_dict\", \"text\"], axis=1, inplace=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "def process_chunk(chunk, i, business_df):\n",
    "    # process the chunk\n",
    "    restaurant_b_ids = list(business_df[\"business_id\"].unique())\n",
    "    chunk = chunk[[\"business_id\", \"stars\", \"useful\", \"text\", \"date\"]]\n",
    "    filtered_df = chunk[chunk[\"business_id\"].isin(restaurant_b_ids)]\n",
    "\n",
    "    cleaned_data = clean_data(filtered_df)\n",
    "\n",
    "    batch_size = len(cleaned_data)\n",
    "    record_count = [0, batch_size]\n",
    "    logging.info(f\"Chunk: {i} batch size: {batch_size}\")\n",
    "\n",
    "    # validate the data\n",
    "    # validate_data(cleaned_data)\n",
    "    cleaned_data = cleaned_data.assign(\n",
    "        sentiment=cleaned_data[\"text\"].progress_apply(\n",
    "            lambda x: get_review_sentiment_stanza(\n",
    "                x,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # cleaned_data = unpack_sentiment(cleaned_data)\n",
    "\n",
    "    # validate the data once again\n",
    "    # validate_data(cleaned_data)\n",
    "\n",
    "    # Write file to storage\n",
    "    # may also write it into Yelp_db\n",
    "    cleaned_data.to_csv(\n",
    "        f\"D:\\My-Projects\\stonecap\\data\\processed\\\\reviews\\chunk_{i}.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "    logging.info(f\"Processed chunk {i}\")\n",
    "\n",
    "\n",
    "def process_chunk_parallel(args):\n",
    "    chunk, i, business_df = args\n",
    "    process_chunk(chunk, i, business_df)\n",
    "    \n",
    "    # Save checkpoint after processing each chunk\n",
    "    save_checkpoint(i + 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunksize = 10000\n",
    "    # Process the chunks first\n",
    "    last_checkpoint = load_checkpoint()\n",
    "    business_df = pd.read_csv(r\"D:\\My-Projects\\stonecap\\data\\processed\\business.csv\")\n",
    "\n",
    "    df_iter = pd.read_json(\n",
    "        r\"D:\\My-Projects\\stonecap\\data\\raw\\yelp_academic_dataset_review.json\",\n",
    "        lines=True,\n",
    "        chunksize=chunksize,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    \n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Prepare arguments for parallel processing\n",
    "        args_list = [(chunk, i, business_df) for i, chunk in enumerate(df_iter) if i >= last_checkpoint]\n",
    "\n",
    "        # Use ThreadPoolExecutor to process chunks in parallel\n",
    "        for _ in tqdm(executor.map(process_chunk_parallel, args_list), total=len(args_list), desc=\"Processing Chunks\"):\n",
    "            pass\n",
    "    \n",
    "        save_checkpoint(len(df_iter))\n",
    "    # for i, chunk in enumerate(df_iter):\n",
    "    #     if i < last_checkpoint:\n",
    "    #         continue\n",
    "    #     process_chunk(chunk, i, business_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "capstone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
